# Least Squares Method - Detailed Lecture Notes
**CSCA5622 - Module 01**

---

## üìö Overview

This document provides a comprehensive breakdown of the Least Squares Method, covering fundamental concepts essential for understanding linear regression in machine learning. Topics include:

- The effect of scaling on regression coefficients
- The normal equation for multivariate regression  
- Handling singular matrices with the pseudo-inverse
- Error metrics for evaluating regression performance

All concepts are explained with mathematical derivations, intuitive explanations, and practical Python examples.

---

## 1. Scaling Effects on Regression Coefficients

### üîç Core Concept

**Scaling** refers to changing the units or magnitude of your variables. This transformation directly affects regression coefficients, making it crucial to understand for proper model interpretation.

### üí° Real-World Examples

#### Example 1: Housing Prices
- **Original**: `x` = house size in square feet, `y` = price in dollars
- **Scaled**: `x'` = house size in square meters (√∑ 10.764), `y'` = price in thousands of dollars (√∑ 1000)

#### Example 2: Temperature Prediction  
- **Original**: `x` = altitude in meters, `y` = temperature in Celsius
- **Scaled**: `x'` = altitude in kilometers (√∑ 1000), `y'` = temperature in Fahrenheit (√ó 9/5 + 32)

### üßÆ Mathematical Relationships

Given scale factors:
- `r`: scale factor for x (e.g., r = 100 to convert meters ‚Üí centimeters)
- `s`: scale factor for y (e.g., s = 0.000001 to convert dollars ‚Üí millions)

**Transformed variables:**
- `x'` = r √ó x  
- `y'` = s √ó y

**Updated coefficients:**
```
Œ≤‚ÇÅ' = (s/r) √ó Œ≤‚ÇÅ
Œ≤‚ÇÄ' = s √ó (»≥ - Œ≤‚ÇÅ √ó xÃÑ)
```

### üìä Practical Python Example

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression

# Original data: house size (sq ft) vs price ($)
data = pd.DataFrame({
    'size_sqft': [1000, 1500, 2000, 2500, 3000],
    'price_dollars': [200000, 275000, 350000, 425000, 500000]
})

# Original regression
X_orig = data[['size_sqft']]
y_orig = data['price_dollars']
model_orig = LinearRegression().fit(X_orig, y_orig)

print(f"Original: Œ≤‚ÇÅ = {model_orig.coef_[0]:.2f} $/sqft")
print(f"Original: Œ≤‚ÇÄ = {model_orig.intercept_:.2f} $")

# Scale: sqft ‚Üí sq meters (√∑10.764), dollars ‚Üí thousands (√∑1000)
r = 1/10.764  # x scale factor  
s = 1/1000    # y scale factor

X_scaled = data[['size_sqft']] * r  # Convert to sq meters
y_scaled = data['price_dollars'] * s  # Convert to thousands

model_scaled = LinearRegression().fit(X_scaled, y_scaled)

print(f"Scaled: Œ≤‚ÇÅ = {model_scaled.coef_[0]:.2f} k$/sq_m")
print(f"Scaled: Œ≤‚ÇÄ = {model_scaled.intercept_:.2f} k$")

# Verify relationship: Œ≤‚ÇÅ' = (s/r) √ó Œ≤‚ÇÅ
expected_slope = (s/r) * model_orig.coef_[0]
print(f"Expected slope: {expected_slope:.2f}")
print(f"Actual slope: {model_scaled.coef_[0]:.2f}")
```

### üß† Key Insights

1. **Slope sensitivity**: Smaller input units ‚Üí larger slope coefficients
2. **Intercept scaling**: Only depends on y-scaling, not x-scaling  
3. **Interpretation**: Always check units when comparing coefficients across models

---

## 2. The Normal Equation - Mathematical Foundation

### üîç Goal & Setup

Find the optimal coefficient vector **Œ≤ÃÇ** that minimizes the sum of squared residuals for the linear model:

**y = XŒ≤ + Œµ**

Where:
- `y`: target vector (n √ó 1)
- `X`: design matrix (n √ó p), includes intercept column
- `Œ≤`: coefficient vector (p √ó 1)  
- `Œµ`: error vector (n √ó 1)

### üßÆ Step-by-Step Derivation

#### Step 1: Define the Objective Function
Minimize the **Residual Sum of Squares (RSS)**:
```
RSS(Œ≤) = ||y - XŒ≤||¬≤
```

#### Step 2: Matrix Expansion
```
RSS(Œ≤) = (y - XŒ≤)·µÄ(y - XŒ≤)
       = y·µÄy - y·µÄXŒ≤ - Œ≤·µÄX·µÄy + Œ≤·µÄX·µÄXŒ≤
       = y·µÄy - 2Œ≤·µÄX·µÄy + Œ≤·µÄX·µÄXŒ≤
```

*(Note: y·µÄXŒ≤ = Œ≤·µÄX·µÄy since both are scalars)*

#### Step 3: Take the Gradient
```
‚àáRSS(Œ≤) = d/dŒ≤ [y·µÄy - 2Œ≤·µÄX·µÄy + Œ≤·µÄX·µÄXŒ≤]
        = 0 - 2X·µÄy + 2X·µÄXŒ≤
        = -2X·µÄy + 2X·µÄXŒ≤
```

#### Step 4: Set Gradient to Zero
```
-2X·µÄy + 2X·µÄXŒ≤ = 0
X·µÄXŒ≤ = X·µÄy
```

#### Step 5: Solve for Œ≤
```
Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄy
```

### üß† Geometric Intuition

- **X·µÄX**: Captures correlations between predictors
- **X·µÄy**: Captures how predictors relate to the target
- **Inverse operation**: "Untangles" correlations to isolate individual effects

### üõ† Complete Python Implementation

```python
import numpy as np
import matplotlib.pyplot as plt

def normal_equation_regression(X, y):
    """
    Implement linear regression using the normal equation.
    
    Parameters:
    X: Feature matrix (n_samples, n_features)
    y: Target vector (n_samples,)
    
    Returns:
    beta: Coefficient vector
    """
    # Add intercept column (column of ones)
    X_with_intercept = np.column_stack([np.ones(X.shape[0]), X])
    
    # Normal equation: Œ≤ = (X·µÄX)‚Åª¬πX·µÄy
    XtX = X_with_intercept.T @ X_with_intercept
    Xty = X_with_intercept.T @ y
    beta = np.linalg.inv(XtX) @ Xty
    
    return beta

# Example usage
np.random.seed(42)
X = np.random.randn(100, 2)  # 2 features
true_beta = np.array([3, 1.5, -2])  # [intercept, coef1, coef2]
y = np.column_stack([np.ones(100), X]) @ true_beta + 0.1 * np.random.randn(100)

# Fit using normal equation
estimated_beta = normal_equation_regression(X, y)
print("True coefficients:", true_beta)
print("Estimated coefficients:", estimated_beta)

# Compare with sklearn
from sklearn.linear_model import LinearRegression
sklearn_model = LinearRegression().fit(X, y)
sklearn_beta = np.array([sklearn_model.intercept_] + list(sklearn_model.coef_))
print("Sklearn coefficients:", sklearn_beta)
```

---

## 3. Singular Matrices & The Pseudo-Inverse Solution

### ‚ùì What is a Singular Matrix?

A matrix is **singular** (non-invertible) when:
- Its determinant equals zero
- Its columns are linearly dependent  
- It doesn't have full rank

### üö® When Does This Happen in Regression?

#### Common Scenarios:

1. **Perfect Multicollinearity**:
   ```python
   # Feature 2 is exactly 2x Feature 1
   X = np.array([[1, 2], [2, 4], [3, 6], [4, 8]])
   ```

2. **More Features than Samples**:
   ```python
   # 5 features, only 3 samples ‚Üí underdetermined system
   X = np.random.randn(3, 5)
   ```

3. **Duplicate Features**:
   ```python
   # Accidentally included the same feature twice
   X = np.array([[1, 1], [2, 2], [3, 3]])
   ```

### ‚ö†Ô∏è Problems with Singular Matrices

```python
# This will fail!
try:
    XtX = X.T @ X
    beta = np.linalg.inv(XtX) @ X.T @ y  # Error: Singular matrix
except np.linalg.LinAlgError:
    print("Cannot invert singular matrix!")
```

**Why it fails:**
- No unique solution exists
- Infinite possible coefficient combinations yield the same fit
- Model cannot determine individual feature importance

### üõ† Solution: Moore-Penrose Pseudo-Inverse

The **pseudo-inverse** X‚Å∫ provides the solution with minimum norm when multiple solutions exist.

**Mathematical definition:**
```
Œ≤ÃÇ = X‚Å∫y
```

Where X‚Å∫ is computed via **Singular Value Decomposition (SVD)**:
```
X = UŒ£V·µÄ
X‚Å∫ = VŒ£‚Å∫U·µÄ
```

### üìä Practical Implementation & Comparison

```python
import numpy as np
from sklearn.linear_model import LinearRegression

# Create problematic data (perfect multicollinearity)
n_samples = 100
X1 = np.random.randn(n_samples)
X2 = 2 * X1  # Perfect correlation!
X = np.column_stack([np.ones(n_samples), X1, X2])
y = 3 + 2*X1 + np.random.randn(n_samples) * 0.1

print("Matrix rank:", np.linalg.matrix_rank(X))
print("Matrix shape:", X.shape)
print("Is singular?", np.linalg.matrix_rank(X) < X.shape[1])

# Method 1: Normal equation (will fail)
try:
    beta_normal = np.linalg.inv(X.T @ X) @ X.T @ y
    print("Normal equation succeeded")
except np.linalg.LinAlgError:
    print("Normal equation failed (singular matrix)")

# Method 2: Pseudo-inverse (will work)
beta_pinv = np.linalg.pinv(X) @ y
print("Pseudo-inverse coefficients:", beta_pinv)

# Method 3: Sklearn (uses SVD internally)
sklearn_model = LinearRegression().fit(X[:, 1:], y)  # Exclude manual intercept
sklearn_beta = [sklearn_model.intercept_] + list(sklearn_model.coef_)
print("Sklearn coefficients:", sklearn_beta)

# Verify predictions are the same
y_pred_pinv = X @ beta_pinv
y_pred_sklearn = sklearn_model.predict(X[:, 1:])
print("Predictions match:", np.allclose(y_pred_pinv, y_pred_sklearn))
```

### üß† Key Properties of Pseudo-Inverse

1. **Always exists**: Even for non-square or singular matrices
2. **Minimum norm solution**: Among all solutions, picks the one with smallest ||Œ≤||
3. **Reduces to regular inverse**: When X is invertible, X‚Å∫ = X‚Åª¬π
4. **Used by default**: Most ML libraries use SVD-based methods

---

## 4. Error Metrics for Regression Evaluation

### üìè Understanding Residuals

**Residuals** are the building blocks of all error metrics:
```
e·µ¢ = y·µ¢ - ≈∑·µ¢   (actual - predicted)
```

### üìâ Mean Absolute Error (MAE)

**Formula:**
```
MAE = (1/n) Œ£ |y·µ¢ - ≈∑·µ¢|
```

**Properties:**
- ‚úÖ Same units as target variable
- ‚úÖ Robust to outliers  
- ‚úÖ Intuitive interpretation
- ‚ùå Not differentiable at zero

**When to use:** When you want to penalize all errors equally

### üî≤ Mean Squared Error (MSE)

**Formula:**
```  
MSE = (1/n) Œ£ (y·µ¢ - ≈∑·µ¢)¬≤
```

**Properties:**
- ‚úÖ Differentiable (useful for optimization)
- ‚úÖ Emphasizes large errors
- ‚ùå Units are squared (harder to interpret)
- ‚ùå Sensitive to outliers

**When to use:** When large errors are particularly problematic

### üßÆ Root Mean Squared Error (RMSE)

**Formula:**
```
RMSE = ‚àöMSE = ‚àö[(1/n) Œ£ (y·µ¢ - ≈∑·µ¢)¬≤]
```

**Properties:**
- ‚úÖ Same units as target (like MAE)
- ‚úÖ Penalizes large errors (like MSE)  
- ‚úÖ Most popular regression metric
- ‚ùå Still sensitive to outliers

### üìä Mean Absolute Percentage Error (MAPE)

**Formula:**
```
MAPE = (1/n) Œ£ |y·µ¢ - ≈∑·µ¢|/|y·µ¢| √ó 100%
```

**Properties:**
- ‚úÖ Unit-free (percentage)
- ‚úÖ Easy to interpret
- ‚ùå Undefined when y·µ¢ = 0
- ‚ùå Asymmetric (over-prediction vs under-prediction)

### üéØ Practical Comparison Example

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Generate sample data with outliers
np.random.seed(42)
X = np.linspace(0, 10, 50).reshape(-1, 1)
y_true = 2 + 3*X.flatten() + np.random.normal(0, 1, 50)

# Add some outliers
outlier_indices = [10, 25, 40]
y_true[outlier_indices] += np.array([15, -12, 18])

# Fit model
model = LinearRegression().fit(X, y_true)
y_pred = model.predict(X)

# Calculate all metrics
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

print(f"MAE:  {mae:.3f}")
print(f"MSE:  {mse:.3f}")  
print(f"RMSE: {rmse:.3f}")
print(f"MAPE: {mape:.3f}%")

# Visualize residuals
residuals = y_true - y_pred
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.scatter(X, y_true, alpha=0.7, label='Actual')
plt.plot(X, y_pred, 'r-', label='Predicted')
plt.legend()
plt.title('Predictions vs Actual')

plt.subplot(1, 2, 2)
plt.scatter(y_pred, residuals, alpha=0.7)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.tight_layout()
plt.show()

# Show impact of outliers
print("\n--- Impact of Outliers ---")
mask = np.ones(len(y_true), dtype=bool)
mask[outlier_indices] = False

mae_no_outliers = mean_absolute_error(y_true[mask], y_pred[mask])
mse_no_outliers = mean_squared_error(y_true[mask], y_pred[mask])
rmse_no_outliers = np.sqrt(mse_no_outliers)

print(f"MAE without outliers:  {mae_no_outliers:.3f} (vs {mae:.3f})")
print(f"RMSE without outliers: {rmse_no_outliers:.3f} (vs {rmse:.3f})")
```

---

## 5. Summary & Key Takeaways

| **Concept** | **Key Points** | **Practical Impact** |
|-------------|----------------|---------------------|
| **Scaling** | Changes coefficient values but not model fit | Critical for interpretation and feature comparison |
| **Normal Equation** | Closed-form solution: Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄy | Fast for small datasets, foundations of linear regression |
| **Singular Matrices** | Occur with multicollinearity or p > n | Use pseudo-inverse; indicates data quality issues |
| **Error Metrics** | MAE (robust), MSE (differentiable), RMSE (interpretable) | Choose based on problem requirements and outlier sensitivity |

### üß† Decision Framework

**When to use Normal Equation:**
- Small datasets (n < 10,000)
- Need exact solution
- Learning/debugging purposes

**When to use Pseudo-Inverse:**  
- Multicollinearity present
- More features than samples
- Robust implementation needed

**Error Metric Selection:**
- **MAE**: Outliers present, equal error weighting desired
- **RMSE**: Standard choice, large errors are costly  
- **MAPE**: Need percentage-based interpretation

### üöÄ Next Steps

1. Practice implementing these concepts on real datasets
2. Explore regularization (Ridge/Lasso) for handling multicollinearity
3. Learn gradient descent as an alternative to normal equation
4. Study advanced error metrics (R¬≤, adjusted R¬≤)

---

**üìö Further Reading:**
- Elements of Statistical Learning (Chapter 3)
- Pattern Recognition and Machine Learning (Bishop, Chapter 3)
- Scikit-learn documentation on Linear Models
